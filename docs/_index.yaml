book_path: /federated/_book.yaml
project_path: /federated/_project.yaml
description: <!--no description-->
landing_page:
  custom_css_path: /site-assets/css/style.css
  rows:
  - heading: Collaborative machine learning without centralized training data
    items:
    - classname: devsite-landing-row-50
      description: >
        <!-- Please keep the content of this file in sync with README.md -->
        <p>TensorFlow Federated (TFF) is an open-source framework for collaborative computations
        on distributed data that does not require collecting data at a centralized location.</p>
        <p>The framework has initially been developed to facilitate open research and
        experimentation with
        <a href="https://ai.googleblog.com/2017/04/federated-learning-collaborative.html" class="external">
        Federated Learning</a>, a technology that enables devices owned by end users to
        collaboratively learn a shared prediction model while keeping potentially sensitive
        training data on the devices, thus decoupling the ability to do machine learning from the
        need to collect and store the data in the cloud.</p>
        <p>With the interfaces provided by TFF, developers can test existing federated learning
        algorithms on their models and data, or design new experimental algorithms and run them on
        existing models and data, all within the same open source environment. The framework has
        been designed with composability in mind, and can be used to combine
        independently-developed techniques and components that offer complementary capabilities into
        larger systems.</p>
        <p>Beyond this, TFF also provides a set of building blocks that can be used to implement a
        variety of custom non-learning algorithms, such as analytics over sensitive distributed
        on-device data. The platform consists of two layers:</p>
      list:
      - heading: Federated Learning (FL)
        description: >
          This layer offers a set of high-level interfaces that allow users to plug existing Keras
          or non-Keras machine learning models into the framework, and perform basic tasks, such as
          federated training or evaluation, without having to study the details of federated
          learning algorithms.
        path: /federated/federated_learning
        icon:
          icon_name: chevron_right
          foreground: theme
          background: grey
      - heading: Federated Core (FC)
        description: >
          This layer offers a set of lower-level interfaces that allow users to concisely express
          their own custom federated algorithms by combining TensorFlow with distributed
          communication operators within a strongly-typed functional programming environment. This
          layer also serves as a foundation for the Federated Learning layer.
        path: /federated/federated_core
        icon:
          icon_name: chevron_right
          foreground: theme
          background: grey
      code_block: |
        <pre class = "prettyprint">
        from six.moves import range
        import tensorflow as tf
        import tensorflow_federated as tff
        from tensorflow_federated.python.examples import mnist
        tf.compat.v1.enable_v2_behavior()

        # Load simulation data.
        source, _ = tff.simulation.datasets.emnist.load_data()
        def client_data(n):
          dataset = source.create_tf_dataset_for_client(source.client_ids[n])
          return mnist.keras_dataset_from_emnist(dataset).repeat(10).batch(20)

        # Pick a subset of client devices to participate in training.
        train_data = [client_data(n) for n in range(3)]

        # Grab a single batch of data so that TFF knows what data looks like.
        sample_batch = tf.contrib.framework.nest.map_structure(
            lambda x: x.numpy(), iter(train_data[0]).next())

        # Wrap a Keras model for use with TFF.
        def model_fn():
          return tff.learning.from_compiled_keras_model(
              mnist.create_simple_keras_model(), sample_batch)

        # Simulate a few rounds of training with the selected client devices.
        trainer = tff.learning.build_federated_averaging_process(model_fn)
        state = trainer.initialize()
        for _ in range(5):
          state, metrics = trainer.next(state, train_data)
          print (metrics.loss)

        # 26.4884
        # ...
        # 15.9183
        #
        # The average loss across the client group is decreasing... it works!
        </pre>

  - classname: devsite-landing-row-cards
    items:
    - heading: "Federated Learning: Collaborative Machine Learning without Centralized Training Data"
      image_path: /resources/images/google-research-card-16x9.png
      path: https://ai.googleblog.com/2017/04/federated-learning-collaborative.html
      buttons:
      - label: "Read on Google AI blog"
        path: https://ai.googleblog.com/2017/04/federated-learning-collaborative.html
    - heading: "Making every phone smarter with Federated Learning"
      youtube_id: gbRJPa9d-VU
      buttons:
      - label: Watch the video
        path: https://www.youtube.com/watch?v=gbRJPa9d-VU
    - heading: "TF Federated on GitHub"
      image_path: /resources/images/github-card-16x9.png
      path: https://github.com/tensorflow/federated
      buttons:
      - label: "View on GitHub"
        path: https://github.com/tensorflow/federated
